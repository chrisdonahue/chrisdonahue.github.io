---
title: "Vision Language Models Are Few-Shot Audio Spectrogram Classifiers"
authors:
  - Satvik Dixit
  - Laurie M. Heller
  - Chris Donahue
date: "2024-11-18T00:00:00Z"
doi: ""

publication_types: ["paper-conference"]
publication: "In the *Proceedings of the Audio Imagination Workshop at the 38th Conference on Neural Information Processing Systems*"
publication_short: "In *NeurIPS Audio Imagination Workshop 2024*"

abstract: "We demonstrate that vision language models (VLMs) are capable of recognizing the content in audio recordings when given corresponding spectrogram images. Specifically, we instruct VLMs to perform audio classification tasks in a few-shot setting by prompting them to classify a spectrogram image given example spectrogram images of each class. By carefully designing the spectrogram image representation and selecting good few-shot examples, we show that GPT-4o can achieve 59.00% cross-validated accuracy on the ESC-10 environmental sound classification dataset. Moreover, we demonstrate that VLMs currently outperform the only available commercial audio language model with audio understanding capabilities (Gemini-1.5) on the equivalent audio classification task (59.00% vs. 49.62%), and even perform slightly better than human experts on visual spectrogram classification (73.75% vs. 72.50% on first fold). We envision two potential use cases for these findings: (1) combining the spectrogram and language understanding capabilities of VLMs for audio caption augmentation, and (2) posing visual spectrogram classification as a challenge task for VLMs."

tags:
  - Vision Language Models
  - Audio Classification
  - Spectrograms
  - NeurIPS 2024
featured: false

url_pdf: "https://arxiv.org/pdf/2411.12058.pdf"
url_arxiv: "https://arxiv.org/abs/2411.12058"
---
